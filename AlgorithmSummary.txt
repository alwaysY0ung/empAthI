시각장애인을 위해 NLP와 음성인식,
Vision기술을 활용하여 시각장애인의 웹페이지 이용을 돕는 프로그램.
python의 Selenium으로 웹을 제어하고 조작한다.
Mediapipe 관련 코드와 image captiong, tts 코드와 nlp api 사용 코드는 따로 준비했으므로 생략한다.
TTS는 MeloTTS AI(변환 결과를 mp3로 사용한다)와 gtts(실시간 음성합성)를 함께 사용한다.

1. 실행
2. 웹페이지 열림. 첫 페이지는 https://google.com
3. 현재 열린 웹페이지 개괄적으로 설명한다. (해당하는 주소에 대한 html을 추출해서 nlp에게 전달한다. nlp는 이 html을 기반으로 페이지를 설명한 text를 전달한다. 이를 받아서 tts해서 사용자에게 들려준다)

4. 손동작 인식을 시작한다는 안내음성 mp3를 재생.
Mediapipe로 사용자 동작을 8종류 중 하나로 식별해 입력받고,
5. 손동작 모양으로 다음중, 앞으로 수행할 하나의 기능을 고름.
((1)새로고침 동작 수행_spin, (2)뒤로가기 동작 수행_back, (3)텍스트 입력 동작 수행_okay (4) 클릭 동작 수행_click (5)웹페이지TTS_good (6)이미지 캡셔닝_capture해서 설명하는 동작 수행 (7)프로그램 종료_away)
기능 중 하나를 7가지의 손동작중 하나의 손동작을 나타내어 기능을 고름 각각의 기능은 _서로다른 손동작과 매칭되어있음. 기능 뒤에 _와 함께 작성한 것이 인식된 동작의 명칭

(1)새로고침 동작 수행
selenium으로 웹페이지 새로고침 수행
새로고침했음을 안내하는 mp3 재생

(2) 뒤로가기 동작 수행_back
selenium으로 웹페이지 뒤로가기 동작 수행
뒤로가기했음을 안내하는 mp3 재생

(3)텍스트 입력 동작 수행_okay
"입력할 텍스트를 말해주세요" mp3 재생
녹음 시작.
손동작 인식 시작. 음성인식종료_stop 동작 인식
녹음 종료.
녹음된 파일 STT.
STT된 텍스를 nlp에 전달.
nlp가 "텍스트필드ID, 입력할 텍스트" 전달
이를 ,를 기준으로 파싱
텍스트필드ID와 입력할 텍스트 가지고 selenium이 텍스트 입력 작업 수행
"텍스트 입력 완료했습니다" mp3 재생.

(4) 클릭 동작 수행_click 
"클릭하고싶은 요소를 말해주세요" mp3 재생
녹음 시작.
손동작 인식 시작. 음성인식종료_stop 동작 인식
녹음 종료.
녹음된 파일 STT.
STT된 텍스를 nlp에 전달.
nlp가 "클릭할요소의 ID" 전달
클릭할 요소의 ID를 가지고 selenium이 클릭 작업 수행
"텍스트 입력 완료했습니다" mp3 재생.

(5)웹페이지TTS_good 
BeautifulSoup를 이용해 html에서 사용자에게 노출되는 text만 추출하고
이를 TTS한다
"페이지의 내용 모두 읽어드렸습니다" mp3 재생

(6)이미지 캡셔닝_capture해서 설명하는 동작 수행
"듣고싶은 이미지를 말씀해주세요" mp3 재생
녹음 시작.
손동작 인식 시작. 이미지 캡셔닝_capture동작 인식
녹음 종료.
녹음된 파일 STT.
STT된 텍스를 nlp에 전달.
nlp가 "이미지ID" 전달
이미지 ID를 가지고 selenium이 이미지 다운로드 작업 수행
이미지를 image caption 모델이 분석
그 분석 결과 문장을 nlp에게 전달.
nlp는 이를 풀어서 설명한 결과를 뱉음
이를 STT함
"이미지 설명 완료했습니다" mp3 재생.

(7)프로그램 종료_away
"웹 브라우징 종료하겠습니다."mp3 재생.
프로그램 완전 종료.

각 작업이 수행된 후, 다시 "4. 손동작 인식을 시작한다는 안내음성 mp3를 재생."으로 돌아간다.

단, 위 작업 중 웹페이지를 이동하는 경우가 생긴다면,
4번이 아닌 "3. 현재 열린 웹페이지 개괄적으로 설명한다. (해당하는 주소에 대한 html을 추출해서 nlp에게 전달한다. nlp는 이 html을 기반으로 페이지를 설명한 text를 전달한다. 이를 받아서 tts해서 사용자에게 들려준다)"으로 돌아간다.
